{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy as Web Service\n",
    "\n",
    "There are various way to put a model in production, we'll see how to make it available as a web service hosted in cloud, leveraging **Microsoft Azure Machine Learning Services** and their **Azure Container Instances (ACI)**. \n",
    "\n",
    "The following code is based on the official Microsoft Azure Machine Learning documentation tutorial:  \n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-deploy-models-with-aml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../datasets/20news')\n",
    "DATA_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mult = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.load(DATA_PATH, 'tmp_clas', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, drop_mult=drop_mult)\n",
    "learn.load('final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters should match the one used to train the model.  \n",
    "*In this specific example* most of them are fastai default values, so we can get them from fastai library source code.  \n",
    "In the specific, fastai.text learner internal implementation and related modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mult=0.5\n",
    "dps = default_dropout['classifier'] * drop_mult\n",
    "bptt=70\n",
    "emb_sz=400\n",
    "nh=1150\n",
    "nl=3\n",
    "pad_token=1\n",
    "qrnn=False\n",
    "max_len=70*20\n",
    "lin_ftrs = [50]\n",
    "ps = [0.1]\n",
    "vocab_size = len(data_clas.vocab.itos)\n",
    "n_class = data_clas.c\n",
    "layers = [emb_sz*3] + lin_ftrs + [n_class]\n",
    "ps = [dps[4]] + ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    { \"model\": learn.model.state_dict(), \n",
    "      \"model_params\": {\n",
    "          \"drop_mult\": drop_mult,\n",
    "          \"dps\": dps,\n",
    "          \"bptt\": bptt,\n",
    "          \"emb_sz\": emb_sz,\n",
    "          \"nh\": nh,\n",
    "          \"nl\": nl,\n",
    "          \"pad_token\": pad_token,\n",
    "          \"qrnn\": qrnn,\n",
    "          \"max_len\": max_len,\n",
    "          \"lin_ftrs\": lin_ftrs,\n",
    "          \"ps\": ps,\n",
    "          \"vocab_size\": vocab_size,\n",
    "          \"n_class\": n_class,\n",
    "          \"layers\": layers,\n",
    "          \"ps\": ps},\n",
    "      \"vocab\": data_clas.vocab.itos,\n",
    "      \"classes\": data_clas.classes\n",
    "    }, DATA_PATH/'models'/'final_for_prod.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fastai v1.0, there is a built-in way to perform a similar production export for supported learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class, pred_idx, outputs = learn.predict(\"text to predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./score_cmd.py\n",
    "from fastai.text import *\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class HTMLTextExtractor(html.parser.HTMLParser):\n",
    "    def __init__(self):\n",
    "        super(HTMLTextExtractor, self).__init__()\n",
    "        self.result = [ ]\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.result.append(d)\n",
    "\n",
    "    def get_text(self):\n",
    "        return ''.join(self.result)\n",
    "    \n",
    "    def error(self, message):\n",
    "        return\n",
    "\n",
    "def html_to_text(html):\n",
    "    s = HTMLTextExtractor()    \n",
    "    try:\n",
    "        s.feed(html)\n",
    "        return s.get_text()\n",
    "    except:\n",
    "        return html\n",
    "\n",
    "def custom_tagstrip(x:str) -> str:\n",
    "    \"Remove all html tags in `x`.\"\n",
    "    return html_to_text(x)\n",
    "\n",
    "def load_model(classifier_filename):\n",
    "    \"\"\"Load the classifier and related metadata\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    state = torch.load(Path(classifier_filename).open('rb'), map_location=device)\n",
    "    \n",
    "    if set(state.keys()) == {'model', 'model_params', 'vocab', 'classes'}:\n",
    "        model_state = state['model']\n",
    "        model_params = state['model_params']\n",
    "        itos = state['vocab']\n",
    "        classes = state['classes']\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid model provided.\")\n",
    "            \n",
    "    # Turn it into a string to int mapping (which is what we need)\n",
    "    stoi = collections.defaultdict(lambda:0, {str(v):int(k) for k,v in enumerate(itos)})\n",
    "    \n",
    "    # Get model reference from parameters (even if they are not used at runtime)\n",
    "    model = get_rnn_classifier(bptt=model_params['bptt'],\n",
    "                               max_seq=model_params['max_len'],\n",
    "                               vocab_sz=model_params['vocab_size'], \n",
    "                               emb_sz=model_params['emb_sz'],\n",
    "                               n_hid=model_params['nh'],\n",
    "                               n_layers=model_params['nl'],\n",
    "                               pad_token=model_params['pad_token'],\n",
    "                               layers=model_params['layers'],\n",
    "                               drops=model_params['ps'],\n",
    "                               input_p=model_params['dps'][0],\n",
    "                               weight_p=model_params['dps'][1],\n",
    "                               embed_p=model_params['dps'][2],\n",
    "                               hidden_p=model_params['dps'][3],\n",
    "                               qrnn=model_params['qrnn'])\n",
    "\n",
    "    # Load the trained classifier\n",
    "    model.load_state_dict(model_state)\n",
    "    \n",
    "    # Put the classifier into evaluation mode\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    return stoi, classes, model\n",
    "\n",
    "def predict_text(stoi, model, lang, text):\n",
    "    \"\"\"Do the actual prediction on the text using the model and mapping files passed\"\"\"\n",
    "\n",
    "    # Predictions are done on arrays of input.\n",
    "    # We only have a single input, so turn it into a 1x1 array\n",
    "    texts = [text]\n",
    "\n",
    "    # Tokenize using the fastai wrapper around spaCy\n",
    "    pre_rules = [custom_tagstrip] + defaults.text_pre_rules\n",
    "    tokens = Tokenizer(lang=lang, pre_rules=pre_rules, n_cpus=1).process_all(texts)\n",
    "\n",
    "    # Turn into integers for each word\n",
    "    encoded = np.array([[stoi[o] for o in p] for p in tokens], dtype=np.int64)\n",
    "    \n",
    "    # Turn this array into a tensor\n",
    "    data = torch.from_numpy(encoded)\n",
    "\n",
    "    # Do the predictions\n",
    "    predictions = model(data)\n",
    "    \n",
    "    # Get class probability from classifier predictions\n",
    "    res = F.softmax(predictions[0], -1).detach().cpu().numpy()\n",
    "    \n",
    "    return res[0]\n",
    "\n",
    "def init():\n",
    "    global stoi\n",
    "    global classes\n",
    "    global model\n",
    "    \n",
    "    # Retrieve the path to the model file using the model name\n",
    "    model_path = \"../datasets/20news/models/final_for_prod.pth\"\n",
    "    stoi, classes, model = load_model(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    deser_obj = raw_data\n",
    "    lang = deser_obj['lang']\n",
    "    text = deser_obj['text']\n",
    "    \n",
    "    # Make prediction  \n",
    "    scores = predict_text(stoi, model, lang, text)\n",
    "    pred_class = np.argmax(scores)\n",
    "    \n",
    "    print(f\"Class: {classes[pred_class]} ({scores[pred_class]})\")\n",
    "    \n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    # We have to cast numpy data types (non-serializable) to standard types\n",
    "    return { \"label\": classes[pred_class], \"label_index\": int(pred_class), \"label_score\": float(scores[pred_class]), \"all_scores\": scores.tolist() }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    run({\"lang\": sys.argv[1], \"text\": sys.argv[2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test it by launching from the command line:\n",
    "\n",
    "`python score_cmd.py en \"Example text to classify\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace, Run\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.webservice import AciWebservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to setup your workspace directly from the Azure Portal, or running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_subscription_id = '35d018a1-fd64-4a56-91e9-75f463fbfd0d'\n",
    "azure_resource_group  = 'ps-fastai-rg2'\n",
    "azure_mlworkspace_name  = 'ps-fastai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure Machine Learning Workspace\n",
    "ws = Workspace.create(name=azure_mlworkspace_name,\n",
    "                      subscription_id=azure_subscription_id, \n",
    "                      resource_group=azure_resource_group,\n",
    "                      create_resource_group=True,\n",
    "                      location='westeurope' # Or other supported Azure region   \n",
    "                     )\n",
    "\n",
    "# Save the configuration file\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created the Workspace from the Azure Portal, you can get a reference to it by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = azure_subscription_id, resource_group = azure_resource_group, workspace_name = azure_mlworkspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../datasets/20news/models/final_for_prod.pth'\n",
    "model_name = \"ps-fastai-nlp-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.register(model_path = model_path,\n",
    "                       model_name = model_name,\n",
    "                       tags = {\"key\": \"0.1\"},\n",
    "                       description = \"Pluralsight Fast.AI NLP Classification Model\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.list(ws, name=model_name)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scoring Script for AML Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./score.py\n",
    "from fastai.text import *\n",
    "from azureml.core.model import Model\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class HTMLTextExtractor(html.parser.HTMLParser):\n",
    "    def __init__(self):\n",
    "        super(HTMLTextExtractor, self).__init__()\n",
    "        self.result = [ ]\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.result.append(d)\n",
    "\n",
    "    def get_text(self):\n",
    "        return ''.join(self.result)\n",
    "    \n",
    "    def error(self, message):\n",
    "        return\n",
    "\n",
    "def html_to_text(html):\n",
    "    s = HTMLTextExtractor()    \n",
    "    try:\n",
    "        s.feed(html)\n",
    "        return s.get_text()\n",
    "    except:\n",
    "        return html\n",
    "\n",
    "def custom_tagstrip(x:str) -> str:\n",
    "    \"Remove all html tags in `x`.\"\n",
    "    return html_to_text(x)\n",
    "\n",
    "def load_model(classifier_filename):\n",
    "    \"\"\"Load the classifier and related metadata\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print('USING CUDA-GPU')\n",
    "    else:\n",
    "        print('USING CPU')\n",
    "    \n",
    "    state = torch.load(Path(classifier_filename).open('rb'), map_location=device)\n",
    "    \n",
    "    if set(state.keys()) == {'model', 'model_params', 'vocab', 'classes'}:\n",
    "        model_state = state['model']\n",
    "        model_params = state['model_params']\n",
    "        itos = state['vocab']\n",
    "        classes = state['classes']\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid model provided.\")\n",
    "        \n",
    "    # Turn it into a string to int mapping (which is what we need)\n",
    "    stoi = collections.defaultdict(lambda:0, {str(v):int(k) for k,v in enumerate(itos)})\n",
    "    \n",
    "    # Get model reference from parameters (even if they are not used at runtime)\n",
    "    model = get_rnn_classifier(bptt=model_params['bptt'],\n",
    "                               max_seq=model_params['max_len'],\n",
    "                               vocab_sz=model_params['vocab_size'], \n",
    "                               emb_sz=model_params['emb_sz'],\n",
    "                               n_hid=model_params['nh'],\n",
    "                               n_layers=model_params['nl'],\n",
    "                               pad_token=model_params['pad_token'],\n",
    "                               layers=model_params['layers'],\n",
    "                               drops=model_params['ps'],\n",
    "                               input_p=model_params['dps'][0],\n",
    "                               weight_p=model_params['dps'][1],\n",
    "                               embed_p=model_params['dps'][2],\n",
    "                               hidden_p=model_params['dps'][3],\n",
    "                               qrnn=model_params['qrnn'])\n",
    "\n",
    "    # Load the trained classifier\n",
    "    model.load_state_dict(model_state)\n",
    "    \n",
    "    # Put the classifier into evaluation mode\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    return stoi, classes, model\n",
    "\n",
    "def predict_text(stoi, model, lang, text):\n",
    "    \"\"\"Do the actual prediction on the text using the model and mapping files passed\"\"\"\n",
    "\n",
    "    # Predictions are done on arrays of input.\n",
    "    # We only have a single input, so turn it into a 1x1 array\n",
    "    texts = [text]\n",
    "\n",
    "    # Tokenize using the fastai wrapper around spaCy\n",
    "    pre_rules = [custom_tagstrip] + defaults.text_pre_rules\n",
    "    tokens = Tokenizer(lang=lang, pre_rules=pre_rules, n_cpus=1).process_all(texts)\n",
    "\n",
    "    # Turn into integers for each word\n",
    "    encoded = np.array([[stoi[o] for o in p] for p in tokens], dtype=np.int64)\n",
    "    \n",
    "    # Turn this array into a tensor\n",
    "    data = torch.from_numpy(encoded)\n",
    "\n",
    "    # Do the predictions\n",
    "    predictions = model(data)\n",
    "    \n",
    "    # Get class probability from classifier predictions\n",
    "    res = F.softmax(predictions[0], -1).detach().cpu().numpy()\n",
    "    \n",
    "    return res[0]\n",
    "\n",
    "def init():\n",
    "    global stoi\n",
    "    global classes\n",
    "    global model\n",
    "    \n",
    "    # Retrieve the path to the model file using the model name\n",
    "    model_path = Model.get_model_path(model_name='ps-fastai-nlp-classification')\n",
    "    stoi, classes, model = load_model(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    deser_obj = json.loads(raw_data)\n",
    "    \n",
    "    if not set(deser_obj.keys()) == {'lang', 'text' }:\n",
    "        return { \"error\": \"invalid data\" }\n",
    "    \n",
    "    lang = deser_obj['lang']\n",
    "    text = deser_obj['text']\n",
    "    \n",
    "    # Make prediction  \n",
    "    scores = predict_text(stoi, model, lang, text)\n",
    "    pred_class = np.argmax(scores)\n",
    "    \n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    # We have to cast numpy data types (non-serializable) to standard types\n",
    "    return { \"label\": classes[pred_class], \"label_index\": int(pred_class), \"label_score\": float(scores[pred_class]), \"all_scores\": scores.tolist() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = CondaDependencies()\n",
    "myenv.set_python_version(\"3.6.6\")\n",
    "myenv.add_pip_package(\"torch==1.0.0\")\n",
    "#myenv.add_pip_package(\"https://download.pytorch.org/whl/cu100/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\")\n",
    "myenv.add_pip_package(\"torchvision==0.2.1\")\n",
    "myenv.add_pip_package(\"fastai==1.0.42\")\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./Dockerfile\n",
    "ARG buildtime_scoring_var=30000\n",
    "ENV SCORING_TIMEOUT_MS=$buildtime_scoring_var\n",
    "RUN apt-get -y update && apt-get install -y gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create an Image Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details, see: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.image.containerimage?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_config = ContainerImage.image_configuration(execution_script = \"score.py\",\n",
    "                                                  runtime = \"python\",\n",
    "                                                  conda_file = \"myenv.yml\",\n",
    "                                                  docker_file=\"Dockerfile\",\n",
    "                                                  enable_gpu=False,\n",
    "                                                  description = \"Image with Fast.AI NLP classification model\",\n",
    "                                                  tags = {\"data\": \"20newsgroups\", \"type\": \"classification\"}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image = ContainerImage.create(name = \"myimage\", \n",
    "                              models = [model],\n",
    "                              image_config = image_config,\n",
    "                              workspace = ws\n",
    "                              )\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_build_log_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws.images['myimage:1'].image_build_log_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the image in ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {\"data\": \"20newsgroups\", \"type\": \"classification\"}, \n",
    "                                               description = 'fastai NLP Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ws.images[\"myimage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ContainerImage(ws, id=\"myimage:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "service_name = 'aci-fastai-1'\n",
    "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                            image = image,\n",
    "                                            name = service_name,\n",
    "                                            workspace = ws)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log.rstrip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`az container logs --resource-group <resource-group> --name <containergroup> --container-name <container>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/azure/container-instances/container-instances-get-logs\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/container-instances/container-instances-troubleshooting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
