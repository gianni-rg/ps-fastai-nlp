{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a language model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to going on with this notebook, please run the batch script \"prepare_wiki.bat\", to download and extract Wikipedia content in the desired language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating data for the Language Model (LM). LM's goal is to learn the structure of a language. It learns language by trying to predict the next word given a set of previous words (n-grams). Since the LM does not perform classification, labels can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../lm/data/wiki/' + lang)\n",
    "DATA_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH= DATA_PATH/'models/'\n",
    "MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a smaller dataset to understand the concepts. If you wish to run on the full dataset, define debug as empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = \"_debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
    "\n",
    "1. Clean up extra spaces, tab chars, new line chars and other characters and replace them with standard ones\n",
    "2. Use the awesome [spacy](http://spacy.io) library to tokenize the data. Since spacy does not provide a parallel/multicore version of the tokenizer, the fastai library adds this functionality. This parallel version uses all the cores of your CPUs and runs much faster than the serial version of the spacy tokenizer.\n",
    "\n",
    "Tokenization is the process of splitting the text into separate tokens so that each token can be assigned a unique index. This means we can convert the text into integer indexes our models can use.\n",
    "\n",
    "We use an appropriate chunksize as the tokenization process is memory intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy does support the following languages: 'en', 'de', 'es', 'pt', 'fr', 'it', 'nl'. **If you want to train a LM in an unsupported language, you need to provide and implement a proper tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spacy.load(lang)\n",
    "except OSError:\n",
    "    print(f'spacy tokenization model is not installed for {lang}.')\n",
    "    lang = lang if lang in ['en', 'de', 'es', 'pt', 'fr', 'it', 'nl'] else 'xx'\n",
    "    print(f'Command: python -m spacy download {lang}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "class HTMLTextExtractor(html.parser.HTMLParser):\n",
    "    def __init__(self):\n",
    "        super(HTMLTextExtractor, self).__init__()\n",
    "        self.result = [ ]\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.result.append(d)\n",
    "\n",
    "    def get_text(self):\n",
    "        return ''.join(self.result)\n",
    "    \n",
    "    def error(self, message):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(html):\n",
    "    s = HTMLTextExtractor()    \n",
    "    try:\n",
    "        s.feed(html)\n",
    "        return s.get_text()\n",
    "    except:\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tagstrip(x:str) -> str:\n",
    "    \"Remove all html tags in `x`.\"\n",
    "    return html_to_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1, lang='en'):\n",
    "    if len(df.columns) == 1:\n",
    "        labels = []\n",
    "        texts = f'\\n{BOS} {FLD} 1 ' + df[0].astype(str)\n",
    "    else:\n",
    "        labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "        texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "        for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    \n",
    "    pre_rules = [custom_tagstrip] + defaults.text_pre_rules\n",
    "    tok = Tokenizer(lang=lang, pre_rules=pre_rules, n_cpus=1).process_all(texts)\n",
    "    \n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df, n_lbls=1, lang='en'):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls, lang)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = (DATA_PATH/'tmp')\n",
    "tmp_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(DATA_PATH/f'train{debug}.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tmp_path/f'train_tok{debug}.npy', tok_trn)\n",
    "np.save(tmp_path/f'train_lbl{debug}.npy', trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_joined = [' '.join(o) for o in tok_trn]\n",
    "open(tmp_path/f'train_joined{debug}.txt', 'w', encoding='utf-8').writelines(trn_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(DATA_PATH/f'val{debug}.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_val, val_labels = get_all(df_val, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tmp_path/f'valid_tok{debug}.npy', tok_val)\n",
    "np.save(tmp_path/f'valid_lbl{debug}.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_joined = [' '.join(o) for o in tok_val]\n",
    "open(tmp_path/f'valid_joined{debug}.txt', 'w', encoding='utf-8').writelines(val_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mapping tokens to ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenized texts...\")\n",
    "trn_tok = np.load(tmp_path/f'train_tok{debug}.npy')\n",
    "val_tok = np.load(tmp_path/f'valid_tok{debug}.npy')\n",
    "\n",
    "print(\"Calculating token frequencies...\")\n",
    "freq = Counter(p for o in trn_tok for p in o)\n",
    "\n",
    "print(\"Filtering tokens...\")\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "print(f'Total tokens: {len(itos)-2}+2')\n",
    "\n",
    "print(f'Mapping training set...')\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in trn_tok])\n",
    "np.save(tmp_path/f'train_ids{debug}.npy', trn_lm)\n",
    "\n",
    "print(f'Mapping validation set...')\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in val_tok])\n",
    "np.save(tmp_path/f'valid_ids{debug}.npy', val_lm)\n",
    "\n",
    "print(f'Saving token-id map...')\n",
    "pickle.dump(itos, open(tmp_path/f'itos{debug}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _partition_cols(a,idxs):\n",
    "  i=0\n",
    "  for idx in idxs:\n",
    "      yield a[i:idx]\n",
    "      i=idx\n",
    "  yield a[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_cols(a,idxs): return list(_partition_cols(a,idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_flds(t, fld_id):\n",
    "    t = np.array(t)\n",
    "    idxs = np.nonzero(t==fld_id)[0]\n",
    "    parts = partition_cols(t,idxs)[1:]\n",
    "    reversed = np.concatenate([np.concatenate([o[:2],o[:1:-1]]) for o in parts[::-1]])\n",
    "    return reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_trn_path = tmp_path/f'train_ids_bwd{debug}.npy'\n",
    "bwd_val_path = tmp_path/f'valid_ids_bwd{debug}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s: i for i, s in enumerate(itos)}\n",
    "fld_id = stoi[FLD]\n",
    "\n",
    "print(\"Reversing training ids order...\")\n",
    "bwd_trn = np.array([reverse_flds(o, fld_id) for o in trn_lm])\n",
    "np.save(bwd_trn_path, bwd_trn)\n",
    "\n",
    "print(\"Reversing validation ids order...\")\n",
    "bwd_val = np.array([reverse_flds(o, fld_id) for o in val_lm])\n",
    "np.save(bwd_val_path, bwd_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-train the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of TextDataBunch for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLMDataBunch.load(DATA_PATH, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, bptt = 70, emb_sz = 400, nh = 1150, nl = 3,\n",
    "                               drop_mult = 0.05, alpha = 2, beta = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_func = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find an optimal learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "wd = 1e-3\n",
    "clip = 0.15\n",
    "cycle_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.clip = clip\n",
    "learn.wd = wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = slice(max_lr/(2.6**4), max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(cyc_len = cycle_len, \n",
    "                    max_lr = lr, # learning rate\n",
    "                    div_factor = 20, # factor to discount from max\n",
    "                    moms = (0.8, 0.7), # momentums\n",
    "                    pct_start = 0.1, # where the peak is at \n",
    "                    wd = wd # weight decay\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained our LM, we can save it for use in transfer-learning scenario for other tasks, such as document classification, sentiment analysis, or anything else related to NLP. In particular, we want to save all the Neural Network **excluding** the last layer, used as dummy classification task (next word prediction) for building the proper language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc_lstm')\n",
    "learn.save('model_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify the quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"model_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"questo Ã¨\"\n",
    "N_WORDS = 15\n",
    "N_SENTENCES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.8) for _ in range(N_SENTENCES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
